{
 "cells": [
  {
   "cell_type": "code",
   "id": "3362a434",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T09:06:35.896575Z",
     "start_time": "2025-01-09T09:06:28.373221Z"
    }
   },
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "len(dataset), dataset[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  1))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "e70a58c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:06:47.023163Z",
     "start_time": "2025-01-09T09:06:46.463138Z"
    }
   },
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e59695a4",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-01-09T09:06:52.410952Z",
     "start_time": "2025-01-09T09:06:52.388518Z"
    }
   },
   "source": [
    "def collate_fn(data):\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "f620d0e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:08:27.389143Z",
     "start_time": "2025-01-09T09:07:20.040925Z"
    }
   },
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese')\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids,\n",
    "           attention_mask=attention_mask,\n",
    "           token_type_ids=token_type_ids)\n",
    "\n",
    "#  [batch_size(对应多少句话), seq_len(数据分词的长度), 768(词编码的维度)]\n",
    "out.last_hidden_state.shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14ee90c07996401dbbd856c67f6d40c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "5d3d02a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:09:14.883760Z",
     "start_time": "2025-01-09T09:09:13.477818Z"
    }
   },
   "source": [
    "#定义下游任务模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            # 抽取特征 (pretrained是上面的预训练模型, 自带callable)\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "        # bert进行情感分类只需要拿特征中的第一个量就能分了\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model()\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2025-01-09T09:19:43.896949Z",
     "start_time": "2025-01-09T09:11:28.704621Z"
    }
   },
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "#训练\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels).sum().item() / len(labels)\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenyun/anaconda3/envs/p3824/lib/python3.8/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7056445479393005 0.5625\n",
      "5 0.6703118085861206 0.5625\n",
      "10 0.6129750609397888 0.9375\n",
      "15 0.6629727482795715 0.625\n",
      "20 0.5514135956764221 0.875\n",
      "25 0.6460531949996948 0.875\n",
      "30 0.5934709906578064 0.8125\n",
      "35 0.5029089450836182 1.0\n",
      "40 0.6054123044013977 0.75\n",
      "45 0.5074395537376404 0.875\n",
      "50 0.5787162184715271 0.75\n",
      "55 0.5047308802604675 0.875\n",
      "60 0.5556312799453735 0.6875\n",
      "65 0.5412320494651794 0.75\n",
      "70 0.47782137989997864 0.8125\n",
      "75 0.4380423426628113 1.0\n",
      "80 0.5225905179977417 0.75\n",
      "85 0.5340051651000977 0.75\n",
      "90 0.522573709487915 0.9375\n",
      "95 0.4779770076274872 0.875\n",
      "100 0.5070357322692871 0.875\n",
      "105 0.4477763772010803 0.875\n",
      "110 0.5038772821426392 0.875\n",
      "115 0.5199684500694275 0.8125\n",
      "120 0.5352334976196289 0.8125\n",
      "125 0.4730210602283478 0.9375\n",
      "130 0.4716801047325134 0.9375\n",
      "135 0.5168660879135132 0.875\n",
      "140 0.45817920565605164 0.875\n",
      "145 0.47916457056999207 0.8125\n",
      "150 0.4517856240272522 0.875\n",
      "155 0.59149569272995 0.625\n",
      "160 0.41767576336860657 0.9375\n",
      "165 0.40440046787261963 0.9375\n",
      "170 0.5440130829811096 0.6875\n",
      "175 0.40506982803344727 0.9375\n",
      "180 0.4062100350856781 0.9375\n",
      "185 0.3756151497364044 1.0\n",
      "190 0.46363329887390137 0.875\n",
      "195 0.398882120847702 1.0\n",
      "200 0.4175533652305603 0.9375\n",
      "205 0.5073635578155518 0.8125\n",
      "210 0.47334975004196167 0.875\n",
      "215 0.38351520895957947 1.0\n",
      "220 0.5299739837646484 0.75\n",
      "225 0.3867051601409912 0.9375\n",
      "230 0.4529232382774353 0.875\n",
      "235 0.3778175413608551 1.0\n",
      "240 0.39076462388038635 0.9375\n",
      "245 0.5528314709663391 0.6875\n",
      "250 0.43743038177490234 0.9375\n",
      "255 0.53361976146698 0.8125\n",
      "260 0.5225021243095398 0.8125\n",
      "265 0.46850669384002686 0.875\n",
      "270 0.4673417806625366 0.875\n",
      "275 0.4129672646522522 0.9375\n",
      "280 0.38162222504615784 1.0\n",
      "285 0.47944772243499756 0.8125\n",
      "290 0.49513131380081177 0.875\n",
      "295 0.4355792999267578 0.9375\n",
      "300 0.3722516596317291 1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "275dd1b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-09T09:32:34.910284Z",
     "start_time": "2025-01-09T09:32:12.674036Z"
    }
   },
   "source": [
    "#测试\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 5:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.84375\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
